{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import logging\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import transformers\n",
    "import jsonlines\n",
    "\n",
    "from utilities import tokenize_and_split_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'kotzeje/lamini_docs.jsonl'\n",
    "use_hf = True\n",
    "\n",
    "model_name = 'EleutherAI/pythia-70m'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the training config and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    'model' : {\n",
    "        'pretrained_name' : model_name,\n",
    "        'max_length' : 2048\n",
    "    },\n",
    "    'datasets' : {\n",
    "        'use_hf' : use_hf,\n",
    "        'path' : dataset_path\n",
    "    },\n",
    "    'verbose' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1238\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 138\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_dataset(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to carry out inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens = 1000, max_output_tokens = 100):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length = max_input_tokens\n",
    "    )\n",
    "\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length = max_output_tokens\n",
    "    )\n",
    "\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Does Lamini have the ability to generate text with a specific level of sentiment or emotional tone, such as positivity or urgency?\n",
      "Correct answer from Lamini docs: Yes, Lamini has the ability to generate text with a specific level of sentiment or emotional tone, such as positivity or urgency. This can be achieved through fine-tuning the language model on specific datasets or by providing prompts that indicate the desired emotional tone. Lamini's natural language generation capabilities allow for the creation of text that conveys a wide range of emotions and sentiments.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "I think you're right.  I think you're right.  I think you're right.  I think you're right.  I think you're right.  I think you're right.  I think you're right.  I think you're right.  I think you're right.  I think you're right\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3\n",
    "trained_model_name = f'lamini_docs_{max_steps}_steps'\n",
    "output_dir = 'output/saved_models/' + trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    learning_rate=1.0e-5,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=max_steps,\n",
    "    per_gpu_train_batch_size=32,\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    overwrite_output_dir=False,\n",
    "    disable_tqdm=False,\n",
    "    eval_steps=120,\n",
    "    save_steps=120,\n",
    "    warmup_steps=1,\n",
    "    per_gpu_eval_batch_size=8,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    optim='adafactor',\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.3084454 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    # model_flops=model_flops,\n",
    "    # total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0%|          | 0/3 [04:20<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 164 at dim 1 (got 126)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\data\\data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\data\\data_collator.py:158\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    156\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 164 at dim 1 (got 126)"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n",
      "177\n",
      "383\n",
      "338\n",
      "189\n",
      "185\n",
      "209\n",
      "185\n",
      "110\n",
      "159\n",
      "176\n",
      "150\n",
      "193\n",
      "182\n",
      "159\n",
      "178\n",
      "174\n",
      "143\n",
      "177\n",
      "165\n",
      "479\n",
      "186\n",
      "479\n",
      "182\n",
      "173\n",
      "172\n",
      "141\n",
      "338\n",
      "479\n",
      "120\n",
      "189\n",
      "177\n",
      "140\n",
      "126\n",
      "209\n",
      "165\n",
      "383\n",
      "209\n",
      "446\n",
      "193\n",
      "344\n",
      "168\n",
      "176\n",
      "158\n",
      "148\n",
      "338\n",
      "182\n",
      "178\n",
      "193\n",
      "189\n",
      "186\n",
      "383\n",
      "178\n",
      "174\n",
      "172\n",
      "159\n",
      "168\n",
      "120\n",
      "182\n",
      "172\n",
      "158\n",
      "185\n",
      "211\n",
      "172\n",
      "338\n",
      "110\n",
      "141\n",
      "211\n",
      "172\n",
      "150\n",
      "186\n",
      "169\n",
      "126\n",
      "182\n",
      "338\n",
      "141\n",
      "209\n",
      "158\n",
      "216\n",
      "148\n",
      "216\n",
      "177\n",
      "344\n",
      "158\n",
      "185\n",
      "164\n",
      "173\n",
      "165\n",
      "159\n",
      "174\n",
      "176\n",
      "148\n",
      "227\n",
      "185\n",
      "140\n",
      "110\n",
      "189\n",
      "120\n",
      "120\n",
      "173\n",
      "185\n",
      "216\n",
      "148\n",
      "189\n",
      "150\n",
      "446\n",
      "178\n",
      "158\n",
      "185\n",
      "141\n",
      "143\n",
      "227\n",
      "158\n",
      "148\n",
      "189\n",
      "168\n",
      "209\n",
      "110\n",
      "479\n",
      "172\n",
      "159\n",
      "158\n",
      "148\n",
      "120\n",
      "446\n",
      "147\n",
      "383\n",
      "193\n",
      "182\n",
      "178\n",
      "165\n",
      "141\n",
      "216\n",
      "174\n",
      "143\n",
      "173\n",
      "158\n",
      "172\n",
      "165\n",
      "168\n",
      "143\n",
      "120\n",
      "176\n",
      "182\n",
      "148\n",
      "150\n",
      "148\n",
      "110\n",
      "185\n",
      "141\n",
      "172\n",
      "173\n",
      "209\n",
      "178\n",
      "150\n",
      "120\n",
      "211\n",
      "344\n",
      "344\n",
      "227\n",
      "227\n",
      "143\n",
      "189\n",
      "182\n",
      "159\n",
      "479\n",
      "164\n",
      "168\n",
      "185\n",
      "169\n",
      "176\n",
      "446\n",
      "140\n",
      "189\n",
      "383\n",
      "178\n",
      "159\n",
      "158\n",
      "178\n",
      "165\n",
      "172\n",
      "140\n",
      "172\n",
      "126\n",
      "164\n",
      "158\n",
      "172\n",
      "189\n",
      "158\n",
      "169\n",
      "216\n",
      "209\n",
      "176\n",
      "110\n",
      "150\n",
      "158\n",
      "148\n",
      "186\n",
      "383\n",
      "147\n",
      "158\n",
      "209\n",
      "148\n",
      "383\n",
      "164\n",
      "158\n",
      "182\n",
      "209\n",
      "178\n",
      "147\n",
      "143\n",
      "120\n",
      "165\n",
      "182\n",
      "143\n",
      "141\n",
      "182\n",
      "174\n",
      "147\n",
      "189\n",
      "173\n",
      "110\n",
      "189\n",
      "344\n",
      "140\n",
      "168\n",
      "479\n",
      "168\n",
      "344\n",
      "182\n",
      "147\n",
      "164\n",
      "169\n",
      "216\n",
      "344\n",
      "164\n",
      "150\n",
      "344\n",
      "169\n",
      "189\n",
      "159\n",
      "159\n",
      "186\n",
      "479\n",
      "479\n",
      "150\n",
      "165\n",
      "338\n",
      "147\n",
      "446\n",
      "344\n",
      "141\n",
      "182\n",
      "172\n",
      "383\n",
      "126\n",
      "158\n",
      "174\n",
      "159\n",
      "143\n",
      "110\n",
      "189\n",
      "126\n",
      "158\n",
      "344\n",
      "110\n",
      "174\n",
      "158\n",
      "177\n",
      "173\n",
      "193\n",
      "216\n",
      "227\n",
      "182\n",
      "158\n",
      "148\n",
      "159\n",
      "143\n",
      "150\n",
      "186\n",
      "216\n",
      "165\n",
      "209\n",
      "150\n",
      "150\n",
      "176\n",
      "168\n",
      "178\n",
      "164\n",
      "159\n",
      "173\n",
      "168\n",
      "140\n",
      "185\n",
      "216\n",
      "174\n",
      "176\n",
      "164\n",
      "186\n",
      "177\n",
      "147\n",
      "143\n",
      "168\n",
      "143\n",
      "143\n",
      "182\n",
      "193\n",
      "193\n",
      "178\n",
      "140\n",
      "159\n",
      "338\n",
      "140\n",
      "186\n",
      "110\n",
      "216\n",
      "172\n",
      "172\n",
      "172\n",
      "176\n",
      "168\n",
      "338\n",
      "169\n",
      "158\n",
      "338\n",
      "141\n",
      "158\n",
      "126\n",
      "140\n",
      "383\n",
      "159\n",
      "189\n",
      "211\n",
      "174\n",
      "169\n",
      "147\n",
      "140\n",
      "143\n",
      "185\n",
      "126\n",
      "209\n",
      "338\n",
      "168\n",
      "168\n",
      "165\n",
      "143\n",
      "159\n",
      "140\n",
      "168\n",
      "168\n",
      "193\n",
      "216\n",
      "186\n",
      "189\n",
      "168\n",
      "168\n",
      "126\n",
      "211\n",
      "177\n",
      "172\n",
      "158\n",
      "479\n",
      "150\n",
      "211\n",
      "174\n",
      "338\n",
      "168\n",
      "140\n",
      "216\n",
      "211\n",
      "177\n",
      "344\n",
      "176\n",
      "172\n",
      "168\n",
      "338\n",
      "164\n",
      "383\n",
      "158\n",
      "147\n",
      "186\n",
      "446\n",
      "209\n",
      "177\n",
      "178\n",
      "479\n",
      "185\n",
      "147\n",
      "169\n",
      "338\n",
      "383\n",
      "141\n",
      "164\n",
      "211\n",
      "158\n",
      "168\n",
      "177\n",
      "172\n",
      "141\n",
      "143\n",
      "126\n",
      "176\n",
      "182\n",
      "168\n",
      "344\n",
      "148\n",
      "148\n",
      "140\n",
      "159\n",
      "173\n",
      "174\n",
      "164\n",
      "120\n",
      "177\n",
      "174\n",
      "164\n",
      "165\n",
      "182\n",
      "147\n",
      "169\n",
      "185\n",
      "141\n",
      "110\n",
      "383\n",
      "344\n",
      "172\n",
      "173\n",
      "173\n",
      "159\n",
      "173\n",
      "159\n",
      "148\n",
      "165\n",
      "158\n",
      "178\n",
      "165\n",
      "172\n",
      "193\n",
      "177\n",
      "209\n",
      "189\n",
      "446\n",
      "159\n",
      "159\n",
      "165\n",
      "182\n",
      "446\n",
      "148\n",
      "110\n",
      "159\n",
      "148\n",
      "185\n",
      "189\n",
      "165\n",
      "140\n",
      "189\n",
      "182\n",
      "120\n",
      "164\n",
      "479\n",
      "147\n",
      "173\n",
      "176\n",
      "446\n",
      "344\n",
      "150\n",
      "147\n",
      "209\n",
      "169\n",
      "158\n",
      "186\n",
      "211\n",
      "126\n",
      "193\n",
      "189\n",
      "193\n",
      "159\n",
      "209\n",
      "169\n",
      "165\n",
      "338\n",
      "172\n",
      "158\n",
      "177\n",
      "120\n",
      "169\n",
      "165\n",
      "193\n",
      "176\n",
      "165\n",
      "158\n",
      "173\n",
      "172\n",
      "165\n",
      "383\n",
      "158\n",
      "148\n",
      "344\n",
      "158\n",
      "148\n",
      "148\n",
      "177\n",
      "120\n",
      "172\n",
      "141\n",
      "211\n",
      "159\n",
      "159\n",
      "185\n",
      "189\n",
      "182\n",
      "344\n",
      "479\n",
      "158\n",
      "446\n",
      "193\n",
      "148\n",
      "185\n",
      "150\n",
      "148\n",
      "338\n",
      "479\n",
      "383\n",
      "479\n",
      "159\n",
      "164\n",
      "209\n",
      "143\n",
      "159\n",
      "177\n",
      "158\n",
      "227\n",
      "186\n",
      "172\n",
      "446\n",
      "182\n",
      "176\n",
      "182\n",
      "164\n",
      "176\n",
      "158\n",
      "120\n",
      "193\n",
      "189\n",
      "165\n",
      "344\n",
      "338\n",
      "178\n",
      "165\n",
      "169\n",
      "172\n",
      "164\n",
      "383\n",
      "446\n",
      "172\n",
      "126\n",
      "148\n",
      "178\n",
      "383\n",
      "148\n",
      "147\n",
      "172\n",
      "182\n",
      "120\n",
      "211\n",
      "193\n",
      "174\n",
      "168\n",
      "159\n",
      "186\n",
      "110\n",
      "193\n",
      "178\n",
      "193\n",
      "148\n",
      "148\n",
      "182\n",
      "216\n",
      "446\n",
      "148\n",
      "126\n",
      "168\n",
      "169\n",
      "158\n",
      "174\n",
      "211\n",
      "168\n",
      "110\n",
      "159\n",
      "211\n",
      "185\n",
      "189\n",
      "176\n",
      "173\n",
      "168\n",
      "158\n",
      "176\n",
      "110\n",
      "182\n",
      "172\n",
      "193\n",
      "178\n",
      "164\n",
      "140\n",
      "172\n",
      "189\n",
      "147\n",
      "164\n",
      "140\n",
      "159\n",
      "147\n",
      "216\n",
      "158\n",
      "185\n",
      "147\n",
      "126\n",
      "168\n",
      "193\n",
      "147\n",
      "168\n",
      "159\n",
      "148\n",
      "143\n",
      "479\n",
      "158\n",
      "168\n",
      "110\n",
      "176\n",
      "178\n",
      "120\n",
      "172\n",
      "158\n",
      "147\n",
      "172\n",
      "159\n",
      "172\n",
      "168\n",
      "159\n",
      "182\n",
      "211\n",
      "177\n",
      "182\n",
      "186\n",
      "344\n",
      "227\n",
      "120\n",
      "185\n",
      "159\n",
      "164\n",
      "227\n",
      "168\n",
      "189\n",
      "126\n",
      "227\n",
      "174\n",
      "182\n",
      "185\n",
      "209\n",
      "211\n",
      "141\n",
      "186\n",
      "186\n",
      "168\n",
      "173\n",
      "227\n",
      "159\n",
      "182\n",
      "172\n",
      "148\n",
      "150\n",
      "174\n",
      "158\n",
      "158\n",
      "182\n",
      "158\n",
      "383\n",
      "173\n",
      "209\n",
      "168\n",
      "209\n",
      "479\n",
      "169\n",
      "178\n",
      "164\n",
      "174\n",
      "174\n",
      "446\n",
      "141\n",
      "173\n",
      "186\n",
      "159\n",
      "177\n",
      "227\n",
      "172\n",
      "143\n",
      "182\n",
      "140\n",
      "110\n",
      "173\n",
      "120\n",
      "140\n",
      "211\n",
      "189\n",
      "177\n",
      "143\n",
      "143\n",
      "182\n",
      "338\n",
      "159\n",
      "165\n",
      "174\n",
      "446\n",
      "159\n",
      "158\n",
      "147\n",
      "479\n",
      "158\n",
      "172\n",
      "126\n",
      "110\n",
      "158\n",
      "110\n",
      "159\n",
      "158\n",
      "211\n",
      "211\n",
      "177\n",
      "446\n",
      "227\n",
      "344\n",
      "158\n",
      "176\n",
      "182\n",
      "158\n",
      "189\n",
      "126\n",
      "158\n",
      "158\n",
      "172\n",
      "178\n",
      "189\n",
      "168\n",
      "126\n",
      "182\n",
      "182\n",
      "158\n",
      "141\n",
      "158\n",
      "189\n",
      "338\n",
      "479\n",
      "178\n",
      "211\n",
      "158\n",
      "182\n",
      "164\n",
      "174\n",
      "126\n",
      "174\n",
      "158\n",
      "189\n",
      "159\n",
      "446\n",
      "446\n",
      "148\n",
      "174\n",
      "338\n",
      "182\n",
      "446\n",
      "159\n",
      "164\n",
      "148\n",
      "159\n",
      "168\n",
      "120\n",
      "338\n",
      "147\n",
      "110\n",
      "120\n",
      "178\n",
      "227\n",
      "211\n",
      "126\n",
      "446\n",
      "177\n",
      "178\n",
      "158\n",
      "172\n",
      "176\n",
      "209\n",
      "148\n",
      "227\n",
      "216\n",
      "182\n",
      "158\n",
      "164\n",
      "227\n",
      "158\n",
      "189\n",
      "110\n",
      "148\n",
      "140\n",
      "185\n",
      "446\n",
      "177\n",
      "158\n",
      "172\n",
      "177\n",
      "479\n",
      "148\n",
      "216\n",
      "168\n",
      "182\n",
      "158\n",
      "193\n",
      "174\n",
      "147\n",
      "126\n",
      "227\n",
      "174\n",
      "446\n",
      "338\n",
      "383\n",
      "216\n",
      "164\n",
      "189\n",
      "168\n",
      "176\n",
      "165\n",
      "148\n",
      "158\n",
      "158\n",
      "158\n",
      "176\n",
      "110\n",
      "209\n",
      "216\n",
      "158\n",
      "143\n",
      "159\n",
      "126\n",
      "141\n",
      "143\n",
      "158\n",
      "169\n",
      "159\n",
      "189\n",
      "193\n",
      "158\n",
      "168\n",
      "189\n",
      "159\n",
      "168\n",
      "185\n",
      "126\n",
      "479\n",
      "193\n",
      "159\n",
      "126\n",
      "178\n",
      "383\n",
      "158\n",
      "158\n",
      "164\n",
      "159\n",
      "182\n",
      "143\n",
      "143\n",
      "168\n",
      "186\n",
      "193\n",
      "344\n",
      "148\n",
      "186\n",
      "216\n",
      "174\n",
      "338\n",
      "174\n",
      "338\n",
      "158\n",
      "189\n",
      "176\n",
      "158\n",
      "169\n",
      "141\n",
      "172\n",
      "158\n",
      "159\n",
      "110\n",
      "186\n",
      "168\n",
      "169\n",
      "189\n",
      "169\n",
      "158\n",
      "185\n",
      "173\n",
      "148\n",
      "182\n",
      "182\n",
      "479\n",
      "168\n",
      "141\n",
      "159\n",
      "182\n",
      "338\n",
      "172\n",
      "172\n",
      "159\n",
      "158\n",
      "120\n",
      "168\n",
      "446\n",
      "150\n",
      "158\n",
      "209\n",
      "158\n",
      "168\n",
      "178\n",
      "126\n",
      "176\n",
      "446\n",
      "126\n",
      "338\n",
      "150\n",
      "383\n",
      "193\n",
      "177\n",
      "172\n",
      "158\n",
      "182\n",
      "189\n",
      "176\n",
      "209\n",
      "143\n",
      "148\n",
      "185\n",
      "227\n",
      "141\n",
      "338\n",
      "164\n",
      "110\n",
      "158\n",
      "159\n",
      "177\n",
      "211\n",
      "158\n",
      "159\n",
      "148\n",
      "182\n",
      "185\n",
      "182\n",
      "168\n",
      "211\n",
      "148\n",
      "227\n",
      "158\n",
      "172\n",
      "178\n",
      "227\n",
      "110\n",
      "193\n",
      "120\n",
      "189\n",
      "338\n",
      "148\n",
      "479\n",
      "148\n",
      "446\n",
      "178\n",
      "169\n",
      "189\n",
      "140\n",
      "209\n",
      "141\n",
      "216\n",
      "165\n",
      "177\n",
      "383\n",
      "174\n",
      "209\n",
      "178\n",
      "141\n",
      "158\n",
      "182\n",
      "140\n",
      "168\n",
      "383\n",
      "168\n",
      "140\n",
      "150\n",
      "120\n",
      "169\n",
      "211\n",
      "182\n",
      "211\n",
      "148\n",
      "159\n",
      "211\n",
      "193\n",
      "182\n",
      "446\n",
      "148\n",
      "176\n",
      "172\n",
      "110\n",
      "126\n",
      "479\n",
      "172\n",
      "169\n",
      "172\n",
      "126\n",
      "209\n",
      "110\n",
      "150\n",
      "185\n",
      "178\n",
      "178\n",
      "126\n",
      "172\n",
      "479\n",
      "446\n",
      "141\n",
      "479\n",
      "186\n",
      "126\n",
      "172\n",
      "227\n",
      "148\n",
      "178\n",
      "172\n",
      "147\n",
      "169\n",
      "120\n",
      "140\n",
      "158\n",
      "344\n",
      "172\n",
      "182\n",
      "189\n",
      "182\n",
      "110\n",
      "140\n",
      "172\n",
      "158\n",
      "148\n",
      "479\n",
      "140\n",
      "189\n",
      "177\n",
      "189\n",
      "211\n",
      "148\n",
      "159\n",
      "159\n",
      "159\n",
      "158\n",
      "383\n",
      "158\n",
      "186\n",
      "446\n",
      "147\n",
      "143\n",
      "141\n",
      "182\n",
      "177\n",
      "186\n",
      "446\n",
      "158\n",
      "168\n",
      "141\n",
      "209\n",
      "150\n",
      "193\n",
      "148\n",
      "344\n",
      "140\n",
      "150\n",
      "148\n",
      "182\n",
      "383\n",
      "189\n",
      "178\n",
      "172\n",
      "186\n",
      "158\n",
      "120\n",
      "148\n",
      "147\n",
      "169\n",
      "193\n",
      "176\n",
      "147\n",
      "216\n",
      "216\n",
      "193\n",
      "140\n",
      "158\n",
      "148\n",
      "172\n",
      "164\n",
      "148\n",
      "150\n",
      "185\n",
      "110\n",
      "189\n",
      "148\n",
      "227\n",
      "168\n",
      "383\n",
      "158\n",
      "164\n",
      "148\n",
      "173\n",
      "227\n",
      "185\n",
      "227\n",
      "338\n",
      "158\n",
      "344\n",
      "140\n",
      "227\n",
      "140\n",
      "159\n",
      "150\n",
      "344\n",
      "446\n",
      "147\n",
      "186\n",
      "172\n",
      "140\n",
      "186\n",
      "479\n",
      "189\n",
      "164\n",
      "383\n",
      "383\n",
      "216\n",
      "189\n",
      "189\n",
      "168\n",
      "110\n",
      "216\n",
      "338\n",
      "159\n",
      "189\n",
      "141\n",
      "227\n",
      "383\n",
      "186\n",
      "182\n",
      "182\n",
      "150\n",
      "193\n",
      "211\n",
      "182\n",
      "383\n",
      "141\n",
      "174\n",
      "189\n",
      "150\n",
      "174\n",
      "148\n",
      "150\n",
      "227\n",
      "165\n",
      "147\n",
      "176\n",
      "182\n",
      "227\n",
      "479\n",
      "173\n",
      "143\n",
      "189\n",
      "158\n",
      "177\n",
      "479\n",
      "150\n",
      "172\n",
      "176\n",
      "158\n",
      "172\n",
      "479\n",
      "148\n",
      "176\n",
      "140\n",
      "216\n",
      "176\n",
      "209\n",
      "168\n",
      "148\n",
      "182\n",
      "169\n",
      "446\n",
      "216\n",
      "209\n",
      "150\n",
      "158\n",
      "189\n",
      "120\n",
      "189\n",
      "177\n",
      "479\n",
      "383\n",
      "150\n",
      "126\n",
      "216\n",
      "158\n",
      "209\n",
      "344\n",
      "338\n",
      "182\n",
      "150\n",
      "148\n",
      "182\n",
      "383\n",
      "189\n",
      "143\n",
      "159\n",
      "211\n",
      "143\n",
      "165\n",
      "216\n",
      "120\n",
      "141\n",
      "227\n",
      "168\n",
      "168\n",
      "172\n",
      "168\n",
      "168\n",
      "159\n",
      "189\n",
      "211\n",
      "174\n",
      "216\n",
      "383\n",
      "169\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset['input_ids']:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
